<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kaitian Chao</title>

    <meta name="author" content="Kaitian Chao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:1400px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kaitian Chao
                </p>
                <p>
		I am a second-year Master student majoring in Robotics at the <a href="https://www.upenn.edu/">University of Pennsylvania</a> affiliated to <a href="https://www.grasp.upenn.edu/">GRASP Laboratory</a>, specializing in <strong>robot learning, control, and 3D computer vision</strong>. I have extensive experience in deep learning and robotics research, with projects spanning VLM and VLA for robot arm fine-manipulation, robotic fish modeling and control, F1tenth autonomous driving, UAV control and navigation, diffusion models, and large language models, all aimed at advancing AI-driven capabilities. My passion lies in leveraging cutting-edge AI and vision technologies to develop super-intelligent robotics and software that can significantly enhance productivity. I am fortunate to work with <a href="https://junyaoshi.github.io/">Junyao Shi</a>, <a href="https://jasonma2016.github.io/">Jason Ma</a> and <a href="https://www.seas.upenn.edu/~dineshj/">Prof. Dinesh Jayaraman</a> at <a href="https://www.grasp.upenn.edu/research-groups/perception-action-learning-group/">PAL lab</a>.
                </p>
                <p>
		Previously, I received my B.S. in Electrical Engineering from <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>, where I worked with <a href="https://www.linkedin.com/in/linxzh/">Xiaozhu Lin</a> and <a href="https://sist.shanghaitech.edu.cn/wangyang4_en/main.htm">Prof. Yang Wang</a> on robotics fish modeling and control. I also spent my junior year as an exchange student at the <a href="https://www.berkeley.edu/">University of California, Berkeley</a> <a href="https://eecs.berkeley.edu/">Electrical Engineering & Computer Sciences</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:chaokt [AT] seas [dot] upenn [dot] edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Kaitian_Chao_resume.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/YourName-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=ziWDwq0AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/KC_Q1015">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Beethoven-Q/">Github</a>&nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/kaitian-chao-q7">LinkedIn</a>&nbsp;/&nbsp;
                  <a href="https://www.grasp.upenn.edu/people/kaitian-chao/">GRASP Lab Profile</a>&nbsp;/&nbsp;
                  <a href="https://www.google.com/maps/place/Philadelphia,+PA">Location: Philadelphia, PA</a>

                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/kaitian_profile.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/kaitian_profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in robot learning, control, and 3D computer vision. My research focuses on developing AI-driven capabilities for robotics applications. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <!-- RESEARCH PAPER TEMPLATE - HIGHLIGHTED -->
    <tr bgcolor="#ffffd0">
      <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
        <video width="350" height="350" muted autoplay loop style="animation: none; filter: none;" onloadeddata="this.playbackRate = 5.0;">
          <source src="images/VLManipulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:4px 8px;width:70%;vertical-align:middle">
        <!-- <a href="https://your-paper-link.com"> -->
          <span class="papertitle">VLManipulation: Vision Language Models as
            Coding Agents for Robotic Manipulation</span>
        </a>
        <br>
        Rujia Yang*, 
        <a href="https://junyaoshi.github.io/">Junyao Shi*</a>,  
        <strong>Kaitian Chao*</strong>,
        <a href="https://www.linkedin.com/in/selinawan/">Selina Bingqing Wan</a>,
        <a href="https://scholar.google.com/citations?user=o67NTxYAAAAJ&hl=en">Aurora Jianing Qian</a>,
        <a href="https://www.cis.upenn.edu/~leijh/">Jiahui Lei</a>,
        <a href="https://alvinwen428.github.io/">Chuan Wen</a>,
        <a href="https://vlongle.github.io/">Long Le</a>,
        <a href="https://www.grasp.upenn.edu/people/yifei-shao/">Yifei Shao</a>,
        <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
        <br>
        (tentative authorship, * means equal contribution)
        <br>
        <em>(Ongoing Project)</em>
        <br>
        <a href="#" onclick="alert('Project page coming soon!'); return false;">project page</a>
        /
        <a href="#" onclick="alert('arXiv paper coming soon!'); return false;">arXiv</a>
        /
        <a href="#" onclick="alert('Code repository coming soon!'); return false;">code</a>
        /
        <a href="#" onclick="alert('Video demo coming soon!'); return false;">video</a>
        <p></p>
        <p>
          VLManipulation is a general-purpose “code-as-policy” framework that uses coding VLMs to compose perception, planning, and control tools for diverse robot manipulation on both tabletop and mobile platforms. By self-evolving its generated programs to gradually improve task capabilities and integrating state-of-the-art VLA models, it achieves strong zero-shot performance, supports active perception can be used for data collection/finetuning, and in some settings surpasses VLA-only systems.
        </p>
      </td>
    </tr>



    <tr bgcolor="#ffffd0">
      <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
        <img src='images/eureka_manip.png' width="350">
      </td>
      <td style="padding:4px 8px;width:70%;vertical-align:middle">
        <!-- <a href="https://your-paper-link.com"> -->
          <span class="papertitle">Eureka for Manipulation: Real-World Dexterous Agent via Large-Scale Reinforcement Learning</span>
        </a>
        <br>
        
        <a href="https://junyaoshi.github.io/">Junyao Shi*</a>,  
        <a href="https://sagnikanupam.com/">Sagnik Anupam*</a>,
        <strong>Kaitian Chao*</strong>,
        <a href="https://anhquanpham.github.io/">Anh-Quan Pham*</a>,
        <a href="https://ggao22.github.io/">George Jiayuan Gao*</a>,
        <a href="https://www.linkedin.com/in/luyang-hu/">Luyang Hu*</a>,
        <a href="https://obastani.github.io/">Osbert Bastani</a>,
        <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
        <br>
        (tentative authorship, * means equal contribution)
        <br>
        <em>(Ongoing Project)</em>
        <br>
        <a href="#" onclick="alert('Project page coming soon!'); return false;">project page</a>
        /
        <a href="#" onclick="alert('arXiv paper coming soon!'); return false;">arXiv</a>
        /
        <a href="#" onclick="alert('Code repository coming soon!'); return false;">code</a>
        /
        <a href="#" onclick="alert('Video demo coming soon!'); return false;">video</a>
        <p></p>
        <p>
          This research extends the Eureka framework to tackle the "reality gap" in dexterous manipulation by introducing a novel real2sim2real pipeline. We empower the Large Language Model (LLM) agent to not only design rewards but also to automatically refine the physical parameters of a simulated digital twin by matching its behavior to real-world demonstrations. This approach creates high-fidelity training environments, enabling robust reinforcement learning and seamless sim-to-real transfer for complex tasks.
        </p>
      </td>
    </tr>



    <tr bgcolor="#ffffd0">
      <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
        <video width="350" height="350" muted autoplay loop style="animation: none; filter: none;" onloadeddata="this.playbackRate = 6.0;">
          <source src="images/DAIML.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:4px 8px;width:70%;vertical-align:middle">
        <!-- <a href="https://your-paper-link.com"> -->
          <span class="papertitle">Learning Flow-Adaptive Dynamic Model for Robotic Fish Swimming in Unknown Background Flow</span>
        </a>
        <br>
        <strong>Kaitian Chao*</strong>,
        <a href="https://www.linkedin.com/in/linxzh/">Xiaozhu Lin*</a>,
        <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/liuxp/">Xiaopei Liu†</a>,
        <a href="https://sist.shanghaitech.edu.cn/wangyang4_en/main.htm">Yang Wang†</a>,
        <br>
        <em>IROS</em>, 2025
        <br>
        * means equal contribution
        <br>
        <a href="#" onclick="alert('Project page coming soon!'); return false;">project page</a>
        /
        <a href="#" onclick="alert('arXiv paper coming soon!'); return false;">arXiv</a>
        /
        <a href="#" onclick="alert('Code repository coming soon!'); return false;">code</a>
        /
        <a href="#" onclick="alert('Video demo coming soon!'); return false;">video</a>
        <p></p>
        <p>
          We present a novel data-driven dynamic modeling framework capable of characterizing the swimming motions of the robotic fish under various background flow conditions without the necessity for explicit flow information. The model is synthesized by an internal model with an adaptive residual acceleration model to effectively isolate and address external flow effects. Notably, the residual model employs the innovative Domain Adversarially Invariant Meta-Learning (DAIML) approach, allowing the framework to adapt to fluctuating and previously unseen background flow scenarios, enhancing its robustness and scalability. Validation through high-fidelity Computational Fluid Dynamics (CFD) simulations demonstrates the framework’s effectiveness in improving the performance of robotic fish across diverse real-world aquatic environments.
        </p>
      </td>
    </tr>

    <!-- <tr bgcolor="#ffffd0"> -->
      <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
        <video width="350" height="350" muted autoplay loop style="animation: none; filter: none;" onloadeddata="this.playbackRate = 1.2;">
          <source src="images/Artificial Lateral Line System.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:4px 8px;width:70%;vertical-align:middle">
        <!-- <a href="https://your-paper-link.com"> --> 
          <span class="papertitle">Ambient Flow Perception of Freely Swimming Robotic Fish  Using an Artificial Lateral Line System</span>
        </a>
        <br>
        Hongru Dai*,
        <a href="https://www.linkedin.com/in/linxzh/">Xiaozhu Lin*</a>,
        <strong>Kaitian Chao</strong>,
        <a href="https://sist.shanghaitech.edu.cn/wangyang4_en/main.htm">Yang Wang†</a>,
        <br>
        <em>ICRA</em>, 2025
        <br>
        * means equal contribution
        <br>
        <a href="#" onclick="alert('Project page coming soon!'); return false;">project page</a>
        /
        <a href="#" onclick="alert('arXiv paper coming soon!'); return false;">arXiv</a>
        /
        <a href="#" onclick="alert('Code repository coming soon!'); return false;">code</a>
        /
        <a href="#" onclick="alert('Video demo coming soon!'); return false;">video</a>
      <p></p>
      <p>
          Inspired by the natural lateral line system(LLS), a flowresponsive organ in fish that plays a crucial role in behaviors such as rheotaxis, this paper introduces the first Artificial Lateral Line System (ALLS)-based ambient flow classifier for robotic fish that allows robotic fish to perceive flow fields while swimming freely. To be specific, using just 5 pressure sensors and 3.5 minutes of swimming data, we trained a Long Short-Term Memory (LSTM) network, achieving a classification accuracy of 81.25% across 8 flow speed categories, ranging from 0.08 m/s to 0.18 m/s. A key innovation of this work is the formulation of ambient flow perception as a classification task, which not only enables the robotic fish to extract meaningful information but also enhances the robustness and generalizability of the perception framework.
      </p>
    </td>
  </tr>

    



          </tbody></table>

          <!-- PROJECTS SECTION -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
                <p>
                  Here are some of my personal and course projects showcasing my technical skills in robotics, machine learning, and software development.
        </p>
      </td>
    </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <!-- PROJECT TEMPLATE - HIGHLIGHTED -->
    <tr bgcolor="#ffffd0">
  <td style="padding:16px;width:20%;vertical-align:middle">
        <video width="350" height="350" muted autoplay loop style="animation: none; filter: none;" onloadeddata="this.playbackRate = 2.0;">
          <source src="images/The_24th_Roboracer_Autonomous_Grand_Prix_Competition.mp4" type="video/mp4">
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- <a href="https://github.com/yourusername/project-repo"> -->
          <span class="papertitle">The 24th Roboracer Autonomous Grand Prix Competition, ICRA 2025</span>
    </a>
    <br>
        <strong>Competition Project</strong> | <em>ICRA</em> | 2025
        <br>
        <a href="#" onclick="alert('GitHub repository coming soon!'); return false;">GitHub</a>
        /
        <a href="#" onclick="alert('Project demo coming soon!'); return false;">Demo</a>
        /
        <a href="#" onclick="alert('Project video coming soon!'); return false;">Video</a>
          <p></p>
          <p>
            We pushed the limits of autonomous racing at the 24th Roboracer Grand Prix at ICRA 2025, a high-stakes, head-to-head competition for 1/10th scale F1 racing cars. Our approach combined meticulous hardware tuning with a hybrid software stack, switching between Pure Pursuit and Model Predictive Control (MPC) to optimize for raw speed and agile control. This strategy proved highly effective, leading us to secure a high ranking against a field of international teams. 
          </p>
        </td>
      </tr>
      
    <!-- PROJECT TEMPLATE - REGULAR -->
    <tr>
        <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
        <video width="350" height="350" muted autoplay loop style="animation: none; filter: none;" onloadeddata="this.playbackRate = 2.0;">
          <source src="images/Drone_vision_motion_planning.mp4" type="video/mp4">
        </td>
        <td style="padding:4px 8px;width:70%;vertical-align:middle">
        <!-- <a href="https://github.com/yourusername/project-repo2"> -->
          <span class="papertitle">Vision-Based Motion Planning for Agile Drone Navigation in Cluttered Environments</span>
          </a>
          <br>
        <strong>Course Project</strong> | <em>MEAM 6200 Advanced Robotics</em> | 2025
        <br>
        <a href="#" onclick="alert('GitHub repository coming soon!'); return false;">GitHub</a>
        /
        <a href="#" onclick="alert('Project demo coming soon!'); return false;">Demo</a>
          <p></p>
          <p>
          This project demonstrates a complete pipeline for autonomous quadrotor navigation in dense, unknown environments. We developed a system that uses onboard vision to build a map, plans the optimal collision-free path using A*, and executes agile maneuvers using a geometric controller based on differential flatness. This integration of perception, planning, and advanced control allows the drone to fly aggressive, time-optimal trajectories with high precision.
          </p>
        </td>
      </tr>

    <tr>
        <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
        <video width="300" height="300" muted autoplay loop style="animation: none; filter: none;" onloadeddata="this.playbackRate = 5.0;">
          <source src="images/MPPI-driven_autonomous_driving.mp4" type="video/mp4">
        </td>
        <td style="padding:4px 8px;width:70%;vertical-align:middle">
        <!-- <a href="https://github.com/yourusername/project-repo2"> -->
          <span class="papertitle">MPPI-driven Autonomous Driving for an F1TENTH Race Car</span>
          </a>
          <br>
        <strong>Course Project</strong> | <em>MEAM 6150 F1/10 Autonomous Racing Cars</em> | 2025
        <br>
        <a href="#" onclick="alert('GitHub repository coming soon!'); return false;">GitHub</a>
        /
        <a href="#" onclick="alert('Project demo coming soon!'); return false;">Demo</a>
          <p></p>
          <p>
          This project showcases a high-performance autonomous driving system for a 1/10th scale F1 racing car, leveraging a Model Predictive Path Integral (MPPI) controller for agile navigation. Using a LiDAR-built SLAM map within a ROS2 framework, the system demonstrates robust, real-time obstacle avoidance in both simulation and on the physical F1TENTH vehicle.
          </p>
        </td>
      </tr>

    <tr bgcolor="#ffffd0">
        <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
          <video width="350" height="350" muted autoplay loop style="animation: none; filter: none;" onloadeddata="this.playbackRate = 5.0;">
            <source src="images/Robot_arm_manipulation_motion_planning.mp4" type="video/mp4">
        </td>
        <td style="padding:4px 8px;width:70%;vertical-align:middle">
          <!-- <a href="https://github.com/yourusername/project-repo2"> -->
            <span class="papertitle">Dynamic Pick-and-Place Motion Planning with 7-DOF Franka Emika Panda robot arm</span>
          </a>
          <br>
          <strong>Course Project</strong> | <em>MEAM 5200 Advanced Robotics</em> | 2025
          <br>
          <a href="#" onclick="alert('GitHub repository coming soon!'); return false;">GitHub</a>
          /
          <a href="#" onclick="alert('Project demo coming soon!'); return false;">Demo</a>
          <p></p>
          <p>
          This project showcases a complete robotics pipeline for a 7-DOF Franka Emika Panda arm to autonomously perform a dynamic pick-and-place task. By integrating ROS2, computer vision, and advanced motion planning, our system ranked high in the course competition by demonstrating high-precision manipulation in stacking both static cubes and dynamic cubes on a rotating platform.


      </p>
    </td>
  </tr>


        <!-- Add your own collaborative projects here using this template:
    <tr>
      <td style="padding:4px;width:30%;vertical-align:middle;text-align:center">
        <img src='images/your-project-image.jpg' width="280">
      </td>
      <td style="padding:4px 8px;width:70%;vertical-align:middle">
        <a href="https://github.com/your-project">
          <span class="papertitle">Your Project Title</span>
        </a>
        <br>
        <strong>Kaitian Chao</strong>, Collaborator Names
        <br>
        <strong>Project Type</strong> | <em>Context</em> | Year
        <br>
        <a href="https://github.com/your-project">GitHub</a> / <a href="demo-link">Demo</a>
        <p></p>
        <p>Project description here.</p>
      </td>
    </tr>
    -->

          </tbody></table>
          
          <!-- MISCELLANEA SECTION - COMMENTED OUT FOR NOW -->
          <!--
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:4px 8px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            
          </tbody></table>
          -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. Last updated: <script>document.write(new Date().toLocaleDateString());</script>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
